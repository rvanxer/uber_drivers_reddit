{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Get the data about the conversations of Uber drivers on their subreddit __https://www.reddit.com/r/uberdrivers/__ using the Python Reddit API Wrapper (**PRAW**).\n",
    "### Classes\n",
    "There are three important classes for this project:\n",
    "* `post`: equivalent to `thread` of UberPeople\n",
    "* `comment`: equivalent to `message`\n",
    "* `author`: equivalent to `user`\n",
    "\n",
    "### Limitations\n",
    "Currently, there seems to be a few important limitations of this API:\n",
    "1. Most data requests return a listing generator with a limit of 1000 objects. This greatly reduces the size of the dataset, although quality may be acceptable.\n",
    "2. Cannot get followers/friends of redditors of this subreddit. This is very crucial as we cannot perform any direct social network analysis.\n",
    "\n",
    "### Process\n",
    "As a result of these limitations, I'm currently employing workarounds, esp. for the first point. Thankfully, this API lets search the top 1000 posts (equivalent of threads in UberPeople) by various measures - top, hottest, newest, most relevant - as defined by Reddit. It also allows querying the subreddit which is particularly useful for our analysis because one can use keywords relevant to the topic (in this case - strikes & organizing behavior of the drivers). I combine these two methods to gather the relevant posts and pickle the obtained objects. I then extract the comment and author data from these post objects and store the three types of objects as pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw             # main PRAW package for requesting data\n",
    "import prawcore         # for a specific exception in requesting data (not really necessary)\n",
    "import pickle           # for storing objects in binary code\n",
    "import pandas as pd     # for storing object details as tables\n",
    "from tqdm import tqdm   # for visual progress bar\n",
    "import datetime as dt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the reddit object\n",
    "# needs username and password for elevated privilege\n",
    "reddit = praw.Reddit(client_id='wejqi7u3P_dE4A',\n",
    "                    client_secret='21iuFXgESnIg4BaUMzSSt6yinAA',\n",
    "                    user_agent='uber_drivers_reddit',\n",
    "                    username='emphasent',\n",
    "                    password='Wje3Rj!ShVZ9dPg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subreddit object\n",
    "sub = reddit.subreddit('uberdrivers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get post objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1000 posts\n",
    "Reddit API only allows getting the first 1000 objects in any request through a listing generator. This is a limitation of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have the highest difference of upvotes and downvotes\n",
    "toppest = list(sub.top(limit=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hottest posts, with hotness = Log(abs(Upvotes-Downvotes)) + (age/45000)\n",
    "# src: https://www.reddit.com/r/explainlikeimfive/comments/1u0q4s/\n",
    "hottest = list(sub.hot(limit=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newest = list(sub.new(limit=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_popular = set(toppest + hottest + newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also get the top 1000 posts of this year\n",
    "cur_year_popular = set(list(sub.top(limit=1000, time_filter='year')) +\\\n",
    "    list(sub.hot(limit=1000)) + list(sub.new(limit=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_popular = all_time_popular.union(cur_year_popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(toppest))\n",
    "print(len(hottest))\n",
    "print(len(newest))\n",
    "print(len(all_time_popular))\n",
    "print(len(cur_year_popular))\n",
    "print(len(total_popular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the data\n",
    "# pickle.dump(total_popular, open('data/all_top_posts.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_by_keyword(subreddit, keywords, limit=1000,\n",
    "                         methods=['relevance','top','hot','new']):\n",
    "    '''Get post objects containing given list of keywords'''\n",
    "    def add_post(map_, generator, word):\n",
    "        '''Add posts from a generator to a map/dict along with its search word'''\n",
    "        i = 0\n",
    "        for post in generator:\n",
    "            i += 1\n",
    "            # if this post has not been observed before, add it to dict\n",
    "            if post not in posts:\n",
    "                map_[post] = set([word])\n",
    "            # if this post has been observed before, add this word to its set\n",
    "            else:\n",
    "                map_[post].add(word)\n",
    "        # return the no. of posts in the generator, just for reference\n",
    "        return i\n",
    "    \n",
    "    posts = {}\n",
    "    for word in keywords:\n",
    "        for method in methods:\n",
    "            # top all-time posts for current method\n",
    "            all_time = subreddit.search(word, sort=method, limit=limit)\n",
    "            num_posts = add_post(posts, all_time, word)\n",
    "            \n",
    "            # if no. of posts in the all-time generator is equal to the limit,\n",
    "            # then there's a chance that some top posts of current year that\n",
    "            # could not make it to the all-time top list can be extracted\n",
    "            if num_posts == limit:\n",
    "                # for the current year only\n",
    "                cur_year = subreddit.search(word, sort=method,\n",
    "                                            limit=limit, time_filter='year')\n",
    "                add_post(posts, cur_year, word)\n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['strike','protest','organize','discrimination','union','frustrated',\n",
    "            'rights','labor','wage','protection','unemployment','covid','corona',\n",
    "            'treat','policy','surge','pricing','support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search posts by keyword\n",
    "\n",
    "# ---------------- CAUTION: SLOW ---------------------\n",
    "\n",
    "# kw_data = get_posts_by_keyword(sub, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_count(posts):\n",
    "    '''Find the counts of keywords in a posts map'''\n",
    "    counts = {}\n",
    "    for post, words in posts.items():\n",
    "        for word in list(words):\n",
    "            if word not in counts:\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in get_keyword_count(kw_data).items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the data\n",
    "# pickle.dump(kw_data, open('data/keywords_top1000.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the top & keyword  posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = {p: set() for p in total_popular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post, words in kw_data.items():\n",
    "    all_posts[post] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the posts data to pickle\n",
    "pickle.dump(all_posts, open('data/posts_top1000_with_keywords.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment & author objects\n",
    "Get the comment and author objects from the stored post objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the posts map from disk\n",
    "all_posts = pickle.load(open('data/posts_top_with_keywords.pickle', 'rb'))\n",
    "\n",
    "# convert to list, removing the info of search keywords\n",
    "all_posts_list = list(all_posts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_authors(posts):\n",
    "    '''Get the comment and author objects from a list of posts'''\n",
    "    comments = []\n",
    "    authors = set()\n",
    "    \n",
    "    # show progress bar for the posts\n",
    "    for post in tqdm(posts):\n",
    "        try:\n",
    "            authors.add(post.author)\n",
    "            for com in post.comments:\n",
    "                comments.append(com)\n",
    "                authors.add(com.author)\n",
    "        except AttributeError as e:\n",
    "            log_error('{}: {}'.format(post, e))\n",
    "    authors.discard(None)\n",
    "    \n",
    "    return comments, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6371/6371 [1:43:56<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# get the comment and author objects from all the posts\n",
    "coms, auths = get_comments_authors(all_posts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these to pickle\n",
    "pickle.dump({'posts': all_posts, 'comments': coms, 'authors': auths},\n",
    "           open('data/obj_of_top_posts.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objects to tables\n",
    "Convert post, comment, and author objects into tables by filtereing a select columns/fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the collected objects\n",
    "obj = pickle.load(open('data/obj_of_top_posts.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6371\n",
      "60814\n",
      "12055\n"
     ]
    }
   ],
   "source": [
    "print(len(obj['posts']))\n",
    "print(len(obj['comments']))\n",
    "print(len(obj['authors']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert objects to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(msg, file='data/data_read_errors.log'):\n",
    "    '''Log the data read errors\n",
    "    (not using `logging` module coz of some error)'''\n",
    "    with open(file, 'a') as f:\n",
    "        f.write('%s: %s\\n' % (dt.datetime.now().isoformat(), msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_to_table(objs, fields):\n",
    "    '''Extract given fields of a list of PRAW objects into a Pandas\n",
    "    dataframe and optionally save it to a file'''\n",
    "    data = []\n",
    "    profile = {f: 0 for f in fields}\n",
    "    for obj in tqdm(objs):\n",
    "        try:\n",
    "            record = {}\n",
    "            for field in fields:\n",
    "                t = time()\n",
    "                record[field] = getattr(obj, field)\n",
    "                profile[field] += time() - t\n",
    "            data.append(record)\n",
    "        except (AttributeError, prawcore.exceptions.NotFound) as e:\n",
    "            log_error('{}: {}'.format(obj, e))\n",
    "    # convert to pandas dataframe    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df, profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6371/6371 [00:00<00:00, 27765.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the normal fields\n",
    "post_df = obj_to_table(list(obj['posts'].keys()),\n",
    "    ['id','created','author_fullname','num_comments','permalink',\n",
    "     'score','title','upvote_ratio','selftext'])\n",
    "\n",
    "post_df[0].to_csv('data/posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 60814/60814 [00:08<00:00, 7228.04it/s]\n"
     ]
    }
   ],
   "source": [
    "com_df = obj_to_table(\n",
    "    obj['comments'],\n",
    "    ['id','link_id','created_utc','author_fullname','is_submitter','parent_id','score','body'])\n",
    "\n",
    "com_df[0].to_csv('data/comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12055/12055 [3:19:23<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 11774.861673355103, 'name': 0.005012989044189453, 'comment_karma': 0.002001047134399414, 'created_utc': 0.0010306835174560547, 'fullname': 0.031147241592407227, 'is_mod': 0.0050811767578125, 'is_gold': 0.0030248165130615234, 'link_karma': 0.0009953975677490234}\n"
     ]
    }
   ],
   "source": [
    "# not reading 'id' because it takes awfully long, not sure why\n",
    "auth_df, profile = obj_to_table(list(obj['authors']),\n",
    "    ['id','name','comment_karma','created_utc','fullname','is_mod','is_gold','link_karma'])\n",
    "\n",
    "auth_df.to_csv('data/authors.csv', index=False)\n",
    "\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Old) Direct read to disk\n",
    "Read thd, msg, auth data directly from the subreddit into dictionaries and directly write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_write_submissions(subreddit, top):\n",
    "    '''Read top submissions and save them to disk \n",
    "    (without storing in memory)'''\n",
    "    # open the files\n",
    "    f_thd = open(files['thd'], 'w', newline='', encoding='utf-8')\n",
    "    f_msg = open(files['msg'], 'w', newline='', encoding='utf-8')\n",
    "    f_auth = open(files['auth'], 'w', newline='', encoding='utf-8')\n",
    "    \n",
    "    # create the writer objects & write the header row\n",
    "    w_thd = csv.writer(f_thd)\n",
    "    w_msg = csv.writer(f_msg)\n",
    "    w_auth = csv.writer(f_auth)\n",
    "    w_thd.writerow(fields['thd'])\n",
    "    w_msg.writerow(fields['msg'])\n",
    "    w_auth.writerow(fields['auth'])\n",
    "    \n",
    "    # iterate over the submissions\n",
    "    post_num = 0\n",
    "    for post in subreddit.top(limit=top):\n",
    "        post_num += 1\n",
    "        print('#%d:' % post_num, post)\n",
    "        try:\n",
    "            # get the thread details\n",
    "            thd = get_obj_info(post, fields['thd'])\n",
    "            if post.author is not None:\n",
    "                thd['auth_id'] = post.author.id\n",
    "                auth = get_obj_info(post.author, fields['auth'])\n",
    "                w_auth.writerow(auth.values())\n",
    "            else:\n",
    "                thd['auth_id'] = ''\n",
    "            w_thd.writerow(thd.values())\n",
    "            \n",
    "            # get the messages/comments\n",
    "            for com in post.comments:\n",
    "                try:\n",
    "                    msg = get_obj_info(com, fields['msg'])\n",
    "                    # get the author details\n",
    "                    if com.author is not None:\n",
    "                        msg['auth_id'] = com.author.id\n",
    "                        auth = get_obj_info(com.author, fields['auth'])\n",
    "                        w_auth.writerow(auth.values())\n",
    "                    else:\n",
    "                        msg['auth_id'] = ''\n",
    "                    w_msg.writerow(msg.values())\n",
    "                except TypeError as e:\n",
    "                    log('msg[{}]:{}'.format(com.id, e))\n",
    "                except AttributeError as e:\n",
    "                    log('msg[{}]:{}'.format(com.id, e))\n",
    "        except Exception as e:\n",
    "            log('post[{}]:unexpected error:{}'.format(post_num, e))\n",
    "    \n",
    "    # close the files\n",
    "    f_thd.close()\n",
    "    f_msg.close()\n",
    "    f_auth.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda6907fd9f99d74449b76bd4c9660fc8ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
